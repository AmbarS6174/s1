{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-05 21:21:04 config.py:1861] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-05 21:21:04 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 02-05 21:21:04 config.py:1020] Defaulting to use mp for distributed inference\n",
      "INFO 02-05 21:21:04 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='simplescaling/s1-32B', speculative_config=None, tokenizer='simplescaling/s1-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=simplescaling/s1-32B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 02-05 21:21:05 multiproc_gpu_executor.py:130] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m INFO 02-05 21:21:09 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m INFO 02-05 21:21:09 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E205 21:30:32.826959745 socket.cpp:1011] [c10d] The client socket has timed out after 600000ms while trying to connect to (127.0.0.1, 57269).\n",
      "[W205 21:30:32.827288459 TCPStore.cpp:358] [c10d] TCP client failed to connect/validate to host 127.0.0.1:57269 - retrying (try=0, timeout=600000ms, delay=88288ms): The client socket has timed out after 600000ms while trying to connect to (127.0.0.1, 57269).\n",
      "Exception raised from throwTimeoutError at ../torch/csrc/distributed/c10d/socket.cpp:1013 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f1c2bf6c446 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x15e04c6 (0x7f1c1714d4c6 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x6029d95 (0x7f1c1bb96d95 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x6029f36 (0x7f1c1bb96f36 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x602a3a4 (0x7f1c1bb973a4 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x5fe8016 (0x7f1c1bb55016 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: c10d::TCPStore::TCPStore(std::string, c10d::TCPStoreOptions const&) + 0x20c (0x7f1c1bb57f7c in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0xd9acdd (0x7f1c2b53acdd in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #8: <unknown function> + 0x4cb474 (0x7f1c2ac6b474 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x4fdcf7]\n",
      "frame #10: _PyObject_MakeTpCall + 0x25b (0x4f747b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #11: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x509d6f]\n",
      "frame #12: PyVectorcall_Call + 0xb9 (0x50a909 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #13: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x507a4c]\n",
      "frame #14: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x4f77e6]\n",
      "frame #15: <unknown function> + 0x4c9ccb (0x7f1c2ac69ccb in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: _PyObject_MakeTpCall + 0x25b (0x4f747b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #17: _PyEval_EvalFrameDefault + 0x56d2 (0x4f3802 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #18: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #19: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #20: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x572387]\n",
      "frame #21: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x4fe324]\n",
      "frame #22: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #23: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #24: PyObject_Call + 0xb8 (0x50a5a8 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #27: PyObject_Call + 0xb8 (0x50a5a8 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #28: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #29: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #30: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4e3 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #31: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #32: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #33: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #34: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #35: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x509d07]\n",
      "frame #36: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #38: PyObject_Call + 0xb8 (0x50a5a8 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #39: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #40: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #41: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #42: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #43: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #44: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #45: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #46: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #47: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4e3 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #48: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x5953a2]\n",
      "frame #49: PyEval_EvalCode + 0x87 (0x5952e7 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #50: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x5c6737]\n",
      "frame #51: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x5c1870]\n",
      "frame #52: PyRun_StringFlags + 0x7b (0x5b995b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #53: PyRun_SimpleStringFlags + 0x3b (0x5b979b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #54: Py_RunMain + 0x26c (0x5b87fc in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #55: Py_BytesMain + 0x39 (0x5885d9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #56: <unknown function> + 0x29d90 (0x7f1c2cd41d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #57: __libc_start_main + 0x80 (0x7f1c2cd41e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #58: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x58848e]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229] Exception in worker VllmWorkerProcess while processing method init_device.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/worker/worker.py\", line 145, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     init_worker_distributed_environment(self.parallel_config, self.rank,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/worker/worker.py\", line 459, in init_worker_distributed_environment\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     init_distributed_environment(parallel_config.world_size, rank,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 992, in init_distributed_environment\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     torch.distributed.init_process_group(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 97, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     func_return = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 1520, in init_process_group\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     store, rank, world_size = next(rendezvous_iterator)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 221, in _tcp_rendezvous_handler\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     store = _create_c10d_store(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 189, in _create_c10d_store\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229]     return TCPStore(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1112951)\u001b[0;0m ERROR 02-05 21:41:26 multiproc_worker_utils.py:229] torch.distributed.DistNetworkError: The client socket has timed out after 600000ms while trying to connect to (127.0.0.1, 57269).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E205 21:41:26.543464039 socket.cpp:1011] [c10d] The client socket has timed out after 600000ms while trying to connect to (127.0.0.1, 57269).\n",
      "[E205 21:41:26.543587920 TCPStore.cpp:346] [c10d] TCP client failed to connect/validate to host 127.0.0.1:57269 - timed out (try=1, timeout=600000ms): The client socket has timed out after 600000ms while trying to connect to (127.0.0.1, 57269).\n",
      "Exception raised from throwTimeoutError at ../torch/csrc/distributed/c10d/socket.cpp:1013 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f1c2bf6c446 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x15e04c6 (0x7f1c1714d4c6 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x6029d95 (0x7f1c1bb96d95 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x6029f36 (0x7f1c1bb96f36 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x602a3a4 (0x7f1c1bb973a4 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x5fe8016 (0x7f1c1bb55016 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: c10d::TCPStore::TCPStore(std::string, c10d::TCPStoreOptions const&) + 0x20c (0x7f1c1bb57f7c in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0xd9acdd (0x7f1c2b53acdd in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #8: <unknown function> + 0x4cb474 (0x7f1c2ac6b474 in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x4fdcf7]\n",
      "frame #10: _PyObject_MakeTpCall + 0x25b (0x4f747b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #11: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x509d6f]\n",
      "frame #12: PyVectorcall_Call + 0xb9 (0x50a909 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #13: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x507a4c]\n",
      "frame #14: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x4f77e6]\n",
      "frame #15: <unknown function> + 0x4c9ccb (0x7f1c2ac69ccb in /home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: _PyObject_MakeTpCall + 0x25b (0x4f747b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #17: _PyEval_EvalFrameDefault + 0x56d2 (0x4f3802 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #18: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #19: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #20: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x572387]\n",
      "frame #21: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x4fe324]\n",
      "frame #22: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #23: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #24: PyObject_Call + 0xb8 (0x50a5a8 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #27: PyObject_Call + 0xb8 (0x50a5a8 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #28: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #29: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #30: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4e3 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #31: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #32: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #33: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #34: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #35: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x509d07]\n",
      "frame #36: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #38: PyObject_Call + 0xb8 (0x50a5a8 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #39: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #40: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #41: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #42: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #43: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #44: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #45: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #46: _PyFunction_Vectorcall + 0x6f (0x4fe13f in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #47: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4e3 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #48: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x5953a2]\n",
      "frame #49: PyEval_EvalCode + 0x87 (0x5952e7 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #50: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x5c6737]\n",
      "frame #51: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x5c1870]\n",
      "frame #52: PyRun_StringFlags + 0x7b (0x5b995b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #53: PyRun_SimpleStringFlags + 0x3b (0x5b979b in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #54: Py_RunMain + 0x26c (0x5b87fc in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #55: Py_BytesMain + 0x39 (0x5885d9 in /home/aiscuser/miniconda3/envs/s1/bin/python)\n",
      "frame #56: <unknown function> + 0x29d90 (0x7f1c2cd41d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #57: __libc_start_main + 0x80 (0x7f1c2cd41e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #58: /home/aiscuser/miniconda3/envs/s1/bin/python() [0x58848e]\n",
      "\n"
     ]
    },
    {
     "ename": "DistNetworkError",
     "evalue": "The client socket has timed out after 600000ms while trying to connect to (127.0.0.1, 57269).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDistNetworkError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimplescaling/s1-32B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensures it uses 2 GPUs\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplescaling/s1-32B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/utils.py:1028\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1025\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m         )\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/entrypoints/llm.py:210\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/engine/llm_engine.py:585\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    583\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/engine/llm_engine.py:347\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    345\u001b[0m     model_config)\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py:26\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/executor_base.py:36\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:113\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Set up signal handlers to shutdown the executor cleanly\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# sometimes gc does not work well\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker(\n\u001b[1;32m    112\u001b[0m     distributed_init_method\u001b[38;5;241m=\u001b[39mdistributed_init_method)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m                   max_concurrent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    116\u001b[0m                   max_parallel_loading_workers)\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:199\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m driver_worker_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[0;32m--> 199\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m driver_worker_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[0;32m--> 199\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py:54\u001b[0m, in \u001b[0;36mResultFuture.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexception\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mDistNetworkError\u001b[0m: The client socket has timed out after 600000ms while trying to connect to (127.0.0.1, 57269)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = LLM(\n",
    "    \"simplescaling/s1-32B\",\n",
    "    tensor_parallel_size=2,  # Ensures it uses 2 GPUs\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(\"simplescaling/s1-32B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-05 21:18:35 config.py:1861] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-05 21:18:41 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 02-05 21:18:41 config.py:1020] Defaulting to use mp for distributed inference\n",
      "INFO 02-05 21:18:41 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='simplescaling/s1-32B', speculative_config=None, tokenizer='simplescaling/s1-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=simplescaling/s1-32B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 02-05 21:18:41 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-05 21:18:41 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 02-05 21:18:41 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m INFO 02-05 21:18:41 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m INFO 02-05 21:18:41 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 02-05 21:18:42 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 02-05 21:18:42 pynccl.py:69] vLLM is using nccl==2.22.3\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m INFO 02-05 21:18:42 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m INFO 02-05 21:18:42 pynccl.py:69] vLLM is using nccl==2.22.3\n",
      "node-0:1111237:1111237 [0] NCCL INFO Bootstrap : Using eth0:10.1.64.89<0>\n",
      "node-0:1111237:1111237 [0] NCCL INFO cudaDriverVersion 12050\n",
      "node-0:1111237:1111237 [0] NCCL INFO NCCL version 2.22.3+cuda12.5\n",
      "node-0:1111622:1111622 [1] NCCL INFO cudaDriverVersion 12050\n",
      "node-0:1111622:1111622 [1] NCCL INFO Bootstrap : Using eth0:10.1.64.89<0>\n",
      "node-0:1111622:1111622 [1] NCCL INFO NCCL version 2.22.3+cuda12.5\n",
      "node-0:1111237:1111237 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "node-0:1111237:1111237 [0] NCCL INFO P2P plugin IBext_v8\n",
      "node-0:1111237:1111237 [0] NCCL INFO NET/IB : No device found.\n",
      "node-0:1111237:1111237 [0] NCCL INFO NET/IB : No device found.\n",
      "node-0:1111237:1111237 [0] NCCL INFO NET/Socket : Using [0]eth0:10.1.64.89<0>\n",
      "node-0:1111237:1111237 [0] NCCL INFO Using network Socket\n",
      "node-0:1111622:1111622 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "node-0:1111622:1111622 [1] NCCL INFO P2P plugin IBext_v8\n",
      "node-0:1111622:1111622 [1] NCCL INFO NET/IB : No device found.\n",
      "node-0:1111622:1111622 [1] NCCL INFO NET/IB : No device found.\n",
      "node-0:1111622:1111622 [1] NCCL INFO NET/Socket : Using [0]eth0:10.1.64.89<0>\n",
      "node-0:1111622:1111622 [1] NCCL INFO Using network Socket\n",
      "node-0:1111622:1111622 [1] NCCL INFO ncclCommInitRank comm 0xcd8fd30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 300000 commId 0xf01b55e90b438710 - Init START\n",
      "node-0:1111237:1111237 [0] NCCL INFO ncclCommInitRank comm 0xcd90f60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 200000 commId 0xf01b55e90b438710 - Init START\n",
      "node-0:1111622:1111622 [1] NCCL INFO Loading topology file /opt/microsoft/ndv4-topo.xml\n",
      "node-0:1111622:1111622 [1] NCCL INFO Loading unnamed topology\n",
      "node-0:1111237:1111237 [0] NCCL INFO Loading topology file /opt/microsoft/ndv4-topo.xml\n",
      "node-0:1111237:1111237 [0] NCCL INFO Loading unnamed topology\n",
      "node-0:1111622:1111622 [1] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:02.0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Could not find real path of /sys/class/pci_bus/ffff:ff/../../ffff:ff:01.0\n",
      "node-0:1111237:1111237 [0] NCCL INFO === System : maxBw 240.0 totalBw 240.0 ===\n",
      "node-0:1111237:1111237 [0] NCCL INFO CPU/0-0 (1/2/-1)\n",
      "node-0:1111237:1111237 [0] NCCL INFO + PCI[5000.0] - NIC/0-0\n",
      "node-0:1111237:1111237 [0] NCCL INFO + PCI[24.0] - PCI/0-ffffff010 (0)\n",
      "node-0:1111237:1111237 [0] NCCL INFO               + PCI[24.0] - GPU/0-200000 (0)\n",
      "node-0:1111237:1111237 [0] NCCL INFO                             + NVL[240.0] - NVS/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO CPU/0-1 (1/2/-1)\n",
      "node-0:1111237:1111237 [0] NCCL INFO + PCI[24.0] - PCI/0-ffffff020 (0)\n",
      "node-0:1111237:1111237 [0] NCCL INFO               + PCI[24.0] - GPU/0-300000 (1)\n",
      "node-0:1111237:1111237 [0] NCCL INFO                             + NVL[240.0] - NVS/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO ==========================================\n",
      "node-0:1111237:1111237 [0] NCCL INFO GPU/200000 :GPU/0-200000 (0/5000.0/LOC) GPU/0-300000 (2/240.0/NVL) NVS/0-0 (1/240.0/NVL) CPU/0-0 (2/24.0/PHB) CPU/0-1 (3/16.0/SYS) \n",
      "node-0:1111237:1111237 [0] NCCL INFO GPU/300000 :GPU/0-200000 (2/240.0/NVL) GPU/0-300000 (0/5000.0/LOC) NVS/0-0 (1/240.0/NVL) CPU/0-0 (3/16.0/SYS) CPU/0-1 (2/24.0/PHB) \n",
      "node-0:1111237:1111237 [0] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff\n",
      "node-0:1111622:1111622 [1] NCCL INFO === System : maxBw 240.0 totalBw 240.0 ===\n",
      "node-0:1111622:1111622 [1] NCCL INFO CPU/0-0 (1/2/-1)\n",
      "node-0:1111622:1111622 [1] NCCL INFO + PCI[5000.0] - NIC/0-0\n",
      "node-0:1111622:1111622 [1] NCCL INFO + PCI[24.0] - PCI/0-ffffff010 (0)\n",
      "node-0:1111622:1111622 [1] NCCL INFO               + PCI[24.0] - GPU/0-200000 (0)\n",
      "node-0:1111622:1111622 [1] NCCL INFO                             + NVL[240.0] - NVS/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO CPU/0-1 (1/2/-1)\n",
      "node-0:1111622:1111622 [1] NCCL INFO + PCI[24.0] - PCI/0-ffffff020 (0)\n",
      "node-0:1111622:1111622 [1] NCCL INFO               + PCI[24.0] - GPU/0-300000 (1)\n",
      "node-0:1111622:1111622 [1] NCCL INFO                             + NVL[240.0] - NVS/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO ==========================================\n",
      "node-0:1111622:1111622 [1] NCCL INFO GPU/200000 :GPU/0-200000 (0/5000.0/LOC) GPU/0-300000 (2/240.0/NVL) NVS/0-0 (1/240.0/NVL) CPU/0-0 (2/24.0/PHB) CPU/0-1 (3/16.0/SYS) \n",
      "node-0:1111622:1111622 [1] NCCL INFO GPU/300000 :GPU/0-200000 (2/240.0/NVL) GPU/0-300000 (0/5000.0/LOC) NVS/0-0 (1/240.0/NVL) CPU/0-0 (3/16.0/SYS) CPU/0-1 (2/24.0/PHB) \n",
      "node-0:1111622:1111622 [1] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff\n",
      "node-0:1111237:1111237 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 12, bw 20.000000/20.000000, type NVL/PIX, sameChannels 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  3 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  4 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  5 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  6 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  7 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  8 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  9 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO 10 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO 11 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 12, bw 40.000000/40.000000, type NVL/PIX, sameChannels 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  3 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  4 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  5 : GPU/0 GPU/1\n",
      "node-0:1111237:1111237 [0] NCCL INFO  6 : GPU/1 GPU/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO  7 : GPU/1 GPU/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO  8 : GPU/1 GPU/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO  9 : GPU/1 GPU/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO 10 : GPU/1 GPU/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO 11 : GPU/1 GPU/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 12, bw 20.000000/20.000000, type NVL/PIX, sameChannels 1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  3 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  4 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  5 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  6 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  7 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  8 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  9 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO 10 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO 11 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 12, bw 40.000000/40.000000, type NVL/PIX, sameChannels 0\n",
      "node-0:1111622:1111622 [1] NCCL INFO  0 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  1 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  2 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  3 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  4 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  5 : GPU/0 GPU/1\n",
      "node-0:1111622:1111622 [1] NCCL INFO  6 : GPU/1 GPU/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO  7 : GPU/1 GPU/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO  8 : GPU/1 GPU/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO  9 : GPU/1 GPU/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO 10 : GPU/1 GPU/0\n",
      "node-0:1111622:1111622 [1] NCCL INFO 11 : GPU/1 GPU/0\n",
      "node-0:1111237:1111237 [0] NCCL INFO comm 0xcd90f60 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n",
      "node-0:1111622:1111622 [1] NCCL INFO comm 0xcd8fd30 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 0 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 12 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 1 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 13 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 2 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 12 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 14 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 3 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 13 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 15 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 2 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 4 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 14 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 16 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 3 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 5 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 15 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 17 : 0 -> 1 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 4 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 6 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 16 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 18 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 5 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 7 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 17 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 19 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 6 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 8 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 18 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 20 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 7 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 9 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 19 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 21 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 8 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 10 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 20 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 22 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 9 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 11 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 21 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Tree 23 : -1 -> 1 -> 0/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 10 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 22 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 11 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Tree 23 : 1 -> 0 -> -1/-1/-1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 00 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 00/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 01 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 01/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 02 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 02/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 03 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 03/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 04 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 04/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 05 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 05/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 06 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 06/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 07 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 07/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 08 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 08/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 09 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 09/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 10 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 10/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 11 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 11/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 12 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 12/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 13 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 13/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 14 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 14/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 15 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 15/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 16 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 16/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 17 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 17/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 18 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 18/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 19 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 19/24 :    0   1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 20/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 20 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 21/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 21 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 22/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 22 : 0 -> 1 -> 0\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 23/24 :    0   1\n",
      "node-0:1111622:1111622 [1] NCCL INFO Ring 23 : 0 -> 1 -> 0\n",
      "node-0:1111622:1111622 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 0/-1/-1->1->-1 [7] 0/-1/-1->1->-1 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] 0/-1/-1->1->-1 [19] 0/-1/-1->1->-1 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 00 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 01 : 1 -> 0 -> 1\n",
      "node-0:1111622:1111622 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 02 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 03 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 04 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 05 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 06 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 07 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 08 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 09 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 10 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 11 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 12 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 13 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 14 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 15 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 16 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 17 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 18 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 19 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 20 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 21 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 22 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Ring 23 : 1 -> 0 -> 1\n",
      "node-0:1111237:1111237 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] -1/-1/-1->0->1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1\n",
      "node-0:1111237:1111237 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111237:1111237 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC/read\n",
      "node-0:1111622:1111622 [1] NCCL INFO Connected all rings\n",
      "node-0:1111622:1111622 [1] NCCL INFO Connected all trees\n",
      "node-0:1111237:1111237 [0] NCCL INFO Connected all rings\n",
      "node-0:1111237:1111237 [0] NCCL INFO Connected all trees\n",
      "node-0:1111622:1111622 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "node-0:1111622:1111622 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "node-0:1111237:1111237 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "node-0:1111237:1111237 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "node-0:1111237:1111237 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576\n",
      "node-0:1111622:1111622 [1] NCCL INFO ncclCommInitRank comm 0xcd8fd30 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 300000 commId 0xf01b55e90b438710 - Init COMPLETE\n",
      "node-0:1111237:1111237 [0] NCCL INFO ncclCommInitRank comm 0xcd90f60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 200000 commId 0xf01b55e90b438710 - Init COMPLETE\n",
      "node-0:1111622:1111622 [1] NCCL INFO Init timings: rank 1 nranks 2 total 1.08 (kernels 0.65, bootstrap 0.08, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.24, rest 0.02)\n",
      "node-0:1111237:1111237 [0] NCCL INFO Init timings: rank 0 nranks 2 total 1.08 (kernels 0.53, bootstrap 0.20, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.25, rest 0.02)\n",
      "INFO 02-05 21:18:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m INFO 02-05 21:18:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 02-05 21:18:44 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ff655eeae90>, local_subscribe_port=38739, remote_subscribe_port=None)\n",
      "INFO 02-05 21:18:44 model_runner.py:1072] Starting to load model simplescaling/s1-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m INFO 02-05 21:18:44 model_runner.py:1072] Starting to load model simplescaling/s1-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/worker/worker.py\", line 152, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1074, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     return loader.load_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 332, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     model = _initialize_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 100, in _initialize_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 430, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     self.model = Qwen2Model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 126, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 279, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 509, in make_layers\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 510, in <listcomp>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 281, in <lambda>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     lambda prefix: Qwen2DecoderLayer(config=config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 201, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     self.mlp = Qwen2MLP(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 69, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 424, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     super().__init__(input_size=input_size,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 304, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     self.quant_method.create_weights(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 122, in create_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]   File \"/home/aiscuser/miniconda3/envs/s1/lib/python3.10/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1111622)\u001b[0;0m ERROR 02-05 21:18:44 multiproc_worker_utils.py:229] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 1 has a total capacity of 79.15 GiB of which 228.19 MiB is free. Process 1872634 has 2.51 GiB memory in use. Process 3100833 has 65.28 GiB memory in use. Process 3272629 has 11.10 GiB memory in use. Of the allocated memory 9.94 GiB is allocated by PyTorch, and 6.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 228.19 MiB is free. Process 1872634 has 2.51 GiB memory in use. Process 3095568 has 65.28 GiB memory in use. Process 3270000 has 11.10 GiB memory in use. Of the allocated memory 9.94 GiB is allocated by PyTorch, and 6.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Decide how often to ignore end-of-thinking token\u001b[39;00m\n\u001b[1;32m      7\u001b[0m NUM_IGNORE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimplescaling/s1-32B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplescaling/s1-32B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/utils.py:1028\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1025\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m         )\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/entrypoints/llm.py:210\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/engine/llm_engine.py:585\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    583\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/engine/llm_engine.py:347\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    345\u001b[0m     model_config)\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py:26\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/executor_base.py:36\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:114\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker(\n\u001b[1;32m    112\u001b[0m     distributed_init_method\u001b[38;5;241m=\u001b[39mdistributed_init_method)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_concurrent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_parallel_loading_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:195\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m worker_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    190\u001b[0m     worker\u001b[38;5;241m.\u001b[39mexecute_method(method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m    192\u001b[0m ]\n\u001b[1;32m    194\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 195\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    199\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/worker/worker.py:152\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/worker/model_runner.py:1074\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1072\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m   1077\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1078\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py:12\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     11\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(vllm_config\u001b[38;5;241m.\u001b[39mload_config)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:332\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[0;32m--> 332\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_weights(model_config, model))\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:100\u001b[0m, in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix)\u001b[0m\n\u001b[1;32m     97\u001b[0m all_params \u001b[38;5;241m=\u001b[39m [param\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m signatures\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# new-style model class\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m registered from out of tree and it is used for new vLLM version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck https://docs.vllm.ai/en/latest/design/class_hierarchy.html \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the design and update the model class accordingly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(msg)\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:430\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config \u001b[38;5;241m=\u001b[39m lora_config\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config \u001b[38;5;241m=\u001b[39m quant_config\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/compilation/decorators.py:126\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# will handle the compilation, so we don't need to do anything here.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_compile \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mVLLM_TORCH_COMPILE_LEVEL \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    130\u001b[0m         CompilationLevel\u001b[38;5;241m.\u001b[39mNO_COMPILATION, CompilationLevel\u001b[38;5;241m.\u001b[39mDYNAMO_AS_IS\n\u001b[1;32m    131\u001b[0m     ] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m supports_dynamo()\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:279\u001b[0m, in \u001b[0;36mQwen2Model.__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m PPMissingLayer()\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_empty_intermediate_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    289\u001b[0m     make_empty_intermediate_tensors_factory(\n\u001b[1;32m    290\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m], config\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:509\u001b[0m, in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    505\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    506\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    507\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    508\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 509\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    510\u001b[0m         maybe_offload_to_cpu(layer_fn(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    512\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:510\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    505\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    506\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    507\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    508\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    509\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m--> 510\u001b[0m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    512\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:281\u001b[0m, in \u001b[0;36mQwen2Model.__init__.<locals>.<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m PPMissingLayer()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m make_layers(\n\u001b[1;32m    280\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_hidden_layers,\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    285\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_empty_intermediate_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    289\u001b[0m     make_empty_intermediate_tensors_factory(\n\u001b[1;32m    290\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m], config\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:201\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    189\u001b[0m rope_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m Qwen2Attention(\n\u001b[1;32m    191\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    192\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.self_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m )\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2MLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    209\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    211\u001b[0m                                         eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:69\u001b[0m, in \u001b[0;36mQwen2MLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, quant_config, prefix)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     62\u001b[0m     hidden_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj \u001b[38;5;241m=\u001b[39m \u001b[43mMergedColumnParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.gate_up_proj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m RowParallelLinear(\n\u001b[1;32m     77\u001b[0m         intermediate_size,\n\u001b[1;32m     78\u001b[0m         hidden_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m         prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.down_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_act \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:424\u001b[0m, in \u001b[0;36mMergedColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, quant_config, prefix)\u001b[0m\n\u001b[1;32m    422\u001b[0m tp_size \u001b[38;5;241m=\u001b[39m get_tensor_model_parallel_world_size()\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(output_size \u001b[38;5;241m%\u001b[39m tp_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output_size \u001b[38;5;129;01min\u001b[39;00m output_sizes)\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgather_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:304\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     output_sizes \u001b[38;5;241m=\u001b[39m [output_size]\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader_v2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mWEIGHT_LOADER_V2_SUPPORTED\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m    316\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size_per_partition,\n\u001b[1;32m    317\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39mparams_dtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:122\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    118\u001b[0m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    119\u001b[0m                    output_partition_sizes: List[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    120\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    121\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_weight_attrs):\n\u001b[0;32m--> 122\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    125\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m    127\u001b[0m     layer\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/s1/lib/python3.10/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 228.19 MiB is free. Process 1872634 has 2.51 GiB memory in use. Process 3095568 has 65.28 GiB memory in use. Process 3270000 has 11.10 GiB memory in use. Of the allocated memory 9.94 GiB is allocated by PyTorch, and 6.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Decide on a token limit for thinking; As the model's max tokens is 32768, 32000 usually ensures there is enough space for the model to still answer\n",
    "MAX_TOKENS_THINKING = 32000\n",
    "# Decide how often to ignore end-of-thinking token\n",
    "NUM_IGNORE = 1\n",
    "\n",
    "model = LLM(\n",
    "    \"simplescaling/s1-32B\",\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    \"simplescaling/s1-32B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
